# Neural_Networks
Building a neural network machine learning model that will be able to predict the success of a charity venture paid by Alphabet soup.

## Alphabet Soup Neural Network Explanation 

- For the machine learning neural network model, I used 2 hidden layers and an outer layer. The neurons on the first hidden layer are two times more than the features the model is analyzing and the second hidden layer has neurons equal to the number of features. I did not use more neurons or extra hidden layers because when I tested these possibilities, the model did not perform any better and actually began to show signs of over fitting. 
- I was not able to get the model above 73% accuracy. The primary way that I was able to increase the performance of the model was finding the best bucketing of application type and classification for the model. I found that the model improved when the number of features generated from these two variables was close to 10 each. When there were too many features, the lower value counts created excess noise in the model making it less efficient, but if too many of the features were bucketed together, there was not enough categories for the model to learn from. Dropping the identification columns along with the STATUS and SPECIAL_CONSIDERATIONS columns was also vital for improving the model's performance. This is because the identification columns do not give the model useful data and the STATUS and SPECIAL_CONSIDERATIONS columns are basically constants so they just add noise to the model. 
- If I were to use a different model then deep neural network, I would use a support vector machine (SVM) model. This model will work well because all the features except for one are binary values. Since the SVM model is a type of binary classifier, it will process this data well and far faster than a deep neural network. In addition, the only non-binary feature can easy be converted to binary values by clumping the range of values into bins. This can be done to potentially improve the SVM model. 
